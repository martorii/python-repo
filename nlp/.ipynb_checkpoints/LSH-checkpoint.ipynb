{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Choosing-the-number-of-planes\" data-toc-modified-id=\"Choosing-the-number-of-planes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Choosing the number of planes</a></span></li><li><span><a href=\"#Getting-the-hash-number-for-a-vector\" data-toc-modified-id=\"Getting-the-hash-number-for-a-vector-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Getting the hash number for a vector</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperlanes-in-vector-spaces\" data-toc-modified-id=\"Hyperlanes-in-vector-spaces-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Hyperlanes in vector spaces</a></span></li><li><span><a href=\"#Using-Hyperplanes-to-split-the-vector-space\" data-toc-modified-id=\"Using-Hyperplanes-to-split-the-vector-space-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Using Hyperplanes to split the vector space</a></span></li><li><span><a href=\"#Encoding-hash-buckets\" data-toc-modified-id=\"Encoding-hash-buckets-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Encoding hash buckets</a></span></li><li><span><a href=\"#Implementing-hash-buckets\" data-toc-modified-id=\"Implementing-hash-buckets-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Implementing hash buckets</a></span></li></ul></li><li><span><a href=\"#Creating-a-hash-table\" data-toc-modified-id=\"Creating-a-hash-table-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating a hash table</a></span></li><li><span><a href=\"#Approximate-K-NN\" data-toc-modified-id=\"Approximate-K-NN-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Approximate K-NN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DIMS = 300\n",
    "N_VECS = 500\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")\n",
    "\n",
    "N_VECS_PER_BUCKET = 20\n",
    "print(f\"We want to split then into buckets with aprox. {N_VECS_PER_BUCKET} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normalize(np.random.rand(N_VECS, N_DIMS), axis=1, norm='l2')\n",
    "print(f\"The vector matrix takes up {train_data.nbytes/1024/1024} Gb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space into $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 500 document vectors into buckets so that every bucket has about $~20$ vectors.\n",
    "* For that we need $\\frac{500}{20}=25$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 25$. Now, we can calculate $n=\\log_{2}25 =  4.64 \\approx 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(25) to have ~20 vectors/bucket.\n",
    "N_PLANES = int(np.floor(np.log2(N_VECS_PER_BUCKET)))\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-4\"></a>\n",
    "\n",
    "## Getting the hash number for a vector\n",
    "\n",
    "For each vector, we need to get a unique number associated to that vector in order to assign it to a \"hash bucket\".\n",
    "\n",
    "### Hyperlanes in vector spaces\n",
    "* In $3$-dimensional vector space, the hyperplane is a regular plane. In $2$ dimensional vector space, the hyperplane is a line.\n",
    "* Generally, the hyperplane is subspace which has dimension $1$ lower than the original vector space has.\n",
    "* A hyperplane is uniquely defined by its normal vector.\n",
    "* Normal vector $n$ of the plane $\\pi$ is the vector to which all vectors in the plane $\\pi$ are orthogonal (perpendicular in $3$ dimensional case).\n",
    "\n",
    "### Using Hyperplanes to split the vector space\n",
    "We can use a hyperplane to split the vector space into $2$ parts.\n",
    "* All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n",
    "* All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n",
    "\n",
    "### Encoding hash buckets\n",
    "* For a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\n",
    "* When the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\n",
    "* Otherwise, if the vector is on the same side as the normal vector, encode it by 1.\n",
    "* If you calculate the dot product with each plane in the same order for every vector, you've encoded each vector's unique hash ID as a binary number, like [0, 1, 1, ... 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-09\"></a>\n",
    "\n",
    "### Implementing hash buckets\n",
    "\n",
    "We've initialized hash table `hashes` for you. It is list of `N_UNIVERSES` matrices, each describes its own hash table. Each matrix has `N_DIMS` rows and `N_PLANES` columns. Every column of that matrix is a `N_DIMS`-dimensional normal vector for each of `N_PLANES` hyperplanes which are used for creating buckets of the particular hash table.\n",
    "\n",
    "*Exercise*: Your task is to complete the function `hash_value_of_vector` which places vector `v` in the correct hash bucket.\n",
    "\n",
    "* First multiply your vector `v`, with a corresponding plane. This will give you a vector of dimension $(1,\\text{N_planes})$.\n",
    "* You will then convert every element in that vector to 0 or 1.\n",
    "* You create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\n",
    "* You then compute the unique number for the vector by iterating over `N_PLANES`\n",
    "* Then you multiply $2^i$ times the corresponding bit (0 or 1).\n",
    "* You will then store that sum in the variable `hash_value`.\n",
    "\n",
    "**Intructions:** Create a hash for the vector in the function below.\n",
    "Use this formula:\n",
    "\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)\n",
    "    dot_product = np.matmul(v, planes)\n",
    "\n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "\n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n",
    "\n",
    "    h = [0 if i<0 else 1 for i in np.squeeze(sign_of_dot_product)]\n",
    "\n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "#     h = np.flatten(h)\n",
    "\n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i\n",
    "        hash_value += np.power(2,i)*h[i]\n",
    "\n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = normalize(np.random.normal(size=(1, N_DIMS)), axis=1, norm='l2')\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a hash table\n",
    "\n",
    "Given that you have a unique number for each vector (or tweet), You now want to create a hash table. You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. This allows you to reduce your search by a significant amount of time.\n",
    "\n",
    "We have given you the `make_hash_table` function, which maps the tweet vectors to a bucket and stores the vector there. It returns the `hash_table` and the `id_table`. The `id_table` allows you know which vector in a certain bucket corresponds to what tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = np.power(2, num_of_planes)\n",
    "    \n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {key: [] for key in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {key: [] for key in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "vec = normalize(np.random.normal(size=(1, N_DIMS)), axis=1, norm='l2')\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(train_data, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(train_data, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate K-NN\n",
    "\n",
    "Implement approximate K nearest neighbors using locality sensitive hashing,\n",
    "to search for documents that are similar to a given document at the\n",
    "index `doc_id`.\n",
    "\n",
    "###### Inputs\n",
    "* `doc_id` is the index into the document list `all_tweets`.\n",
    "* `v` is the document vector for the tweet in `all_tweets` at index `doc_id`.\n",
    "* `planes_l` is the list of planes (the global variable created earlier).\n",
    "* `k` is the number of nearest neighbors to search for.\n",
    "* `num_universes_to_use`: to save time, we can use fewer than the total\n",
    "number of available universes.  By default, it's set to `N_UNIVERSES`,\n",
    "which is $25$ for this assignment.\n",
    "\n",
    "The `approximate_knn` function finds a subset of candidate vectors that\n",
    "are in the same \"hash bucket\" as the input vector 'v'.  Then it performs\n",
    "the usual k-nearest neighbors search on this subset (instead of searching\n",
    "through all 10,000 tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))\n",
    "\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v.flatten(), row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)[::-1]\n",
    "    similarity_l_sorted = np.sort(similarity_l)[::-1]\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[:k]\n",
    "    similarities_k = similarity_l_sorted[:k]\n",
    "\n",
    "    return k_idx, similarities_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(id_tables, hash_tables, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vectors that will be checked as possible nearest neighbor\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # list of document IDs\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        # get the set of planes from the planes_l list, for this particular universe_id\n",
    "        planes = planes_l[universe_id]\n",
    "\n",
    "        # get the hash value of the vector for this set of planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # get the hash table for this particular universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "#         # IMPLEMENT THIS!\n",
    "#         if v in vocab:\n",
    "#             return v\n",
    "\n",
    "        # remove the id of the document that we're searching\n",
    "#         if doc_id in new_ids_to_consider:\n",
    "#             new_ids_to_consider.remove(doc_id)\n",
    "#             print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                embedding = document_vectors_l[i]\n",
    "                document_vector_at_i = vecs_to_consider_l.append(embedding)\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "#     print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l, similarities = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word from search\n",
    "trials = 200\n",
    "for trial in range(trials):\n",
    "    vec_to_search = normalize(np.random.normal(size=(1, N_DIMS)), axis=1, norm='l2')\n",
    "    nearest_neighbor_ids, similarities = approximate_knn(\n",
    "    id_tables, hash_tables, vec_to_search, planes_l, k=2, num_universes_to_use=2)\n",
    "    if trial % 50 == 0:\n",
    "#         print(vec_to_search)\n",
    "#         print(train_data[nearest_neighbor_ids])\n",
    "#         print(nearest_neighbor_ids)\n",
    "#         print(similarities)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_ids = approximate_knn(\n",
    "    id_tables, hash_tables, vec_to_search, planes_l, k=5, num_universes_to_use=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {train_data[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16,
     58
    ]
   },
   "outputs": [],
   "source": [
    "## Create planes and store them as pickle file\n",
    "def create_hash_planes(n_dims, n_planes, n_universes, seed=None):\n",
    "    # Set seed for reproducibility\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    # Create a list with n_universes sets of n_planes of n_dims dimensions\n",
    "    hashing_planes = [np.random.normal(size=(N_DIMS, N_PLANES)) for _ in range(N_UNIVERSES)]\n",
    "    \n",
    "    return hashing_planes\n",
    "\n",
    "def get_all_embeddings():\n",
    "    # Replace this function with get embeddings from database\n",
    "    embeddings = train_data = normalize(np.random.rand(N_VECS, N_DIMS), axis=1, norm='l2')\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = np.power(2, num_of_planes)\n",
    "    \n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {key: [] for key in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {key: [] for key in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table\n",
    "\n",
    "def create_hash_tables(hash_planes, embeddings, n_universes):\n",
    "    # hash_table --> key (): value ()\n",
    "    # id_table --> key(): value()\n",
    "    hash_tables = []\n",
    "    id_tables = []\n",
    "    for universe_id in range(n_universes):  # there are 25 hashes\n",
    "        print('working on hash universe #:', universe_id)\n",
    "        universe_planes = hash_planes[universe_id]\n",
    "        hash_table, id_table = make_hash_table(train_data, universe_planes)\n",
    "        hash_tables.append(hash_table)\n",
    "        id_tables.append(id_table)\n",
    "    return hash_tables, id_tables\n",
    "\n",
    "def create_pandas_universe_hash_table(N_UNIVERSES, id_tables, embeddings):\n",
    "    pd_id_tables = pd.DataFrame()\n",
    "    for universe in range(N_UNIVERSES):\n",
    "        # Get id table for a certain universe\n",
    "        universe_hash_table = id_tables[universe]\n",
    "        # pandas id_tables\n",
    "        pd_id_tables_universe = pd.DataFrame()\n",
    "        for key in universe_hash_table.keys():\n",
    "            # Get hash_ids to identify our vectors (skills)\n",
    "            hash_ids = universe_hash_table[key]\n",
    "            # Get embeddings belonging to this bucket in this universe\n",
    "            universe_key_embeddings = pd.DataFrame(embeddings[hash_ids])\n",
    "            # Create pandas dataframe\n",
    "            pd_id_tables_bucket = pd.DataFrame(data=hash_ids, columns=['hash_id'])\n",
    "            # Assign bucket\n",
    "            pd_id_tables_bucket['hash_bucket'] = key\n",
    "            # Convert to int8\n",
    "            pd_id_tables_bucket['hash_bucket'] = pd.to_numeric(pd_id_tables_bucket['hash_bucket'], downcast='integer')\n",
    "            pd_id_tables_bucket = pd.concat([pd_id_tables_bucket, universe_key_embeddings], axis=1)\n",
    "            # COnvert to float16\n",
    "            for col in pd_id_tables_bucket:\n",
    "                if pd_id_tables_bucket[col].dtype == np.float64:\n",
    "                    pd_id_tables_bucket[col] = pd_id_tables_bucket[col].astype(np.float16)\n",
    "            if not pd_id_tables_bucket.empty:\n",
    "                pd_id_tables_universe = pd.concat([pd_id_tables_universe, pd_id_tables_bucket], axis=0)\n",
    "        pd_id_tables_universe['universe'] = universe\n",
    "        pd_id_tables = pd.concat([pd_id_tables, pd_id_tables_universe], axis=0)\n",
    "    return pd_id_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "# Create planes\n",
    "hash_planes = create_hash_planes(N_DIMS, N_PLANES, N_UNIVERSES)\n",
    "\n",
    "# Get embeddings (skill vectors from the ddbb)\n",
    "embeddings = get_all_embeddings()\n",
    "\n",
    "# Create hashing tables\n",
    "## - hash_tables is a list of n_universes lists with n_buckets dictionaries where\n",
    "## key = 'bucket_id' and value = embedding belongs to this bucket in this universe.\n",
    "## - id_tables is a list of n_universes with n_buckets dictionaries where \n",
    "## key = 'bucket_id' and value = embedding_id that belongs to this bucket in this universe.\n",
    "hash_tables, id_tables = create_hash_tables(hash_planes, embeddings, N_UNIVERSES)\n",
    "\n",
    "pd_id_tables = create_pandas_universe_hash_table(N_UNIVERSES, id_tables, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_id_tables.memory_usage(index=True).sum()/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_id_tables.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
